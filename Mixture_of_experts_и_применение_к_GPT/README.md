# Mixture of Experts.
Выполнили:
* Максим Романов
* Владислав Понизяйкин
* Глеб Дьяконов
* Александра Павлова
* Георгий Гензе

# Содержание
* пункт 1
* пункт 2
* Оценка: почему MoE - это победа
* пункт 4
* Прикладное применение Mixture of Experts в реальной истории.

# Оценка: почему MoE - это победа

> За основу взята статья [Efficient Large Scale Language Modeling with Mixtures of Experts](https://arxiv.org/pdf/2112.10684.pdf)

В этом разделе мы постараемся понять, почему MoE - это хорошо, и почему ему действительно можно найти достойное применение в LLM.

## Напоминание
Для начала, давайте повторим некоторые термины, которые будут важны нам дальше:

* **zero-shot learning** – обучение такой модели, которая сможет корректно работать с задачами, которые раньше не видела
* **few-shot learning** – задача машинного обучения, в которой модель надо преднастроить на тренировочном датасете так, чтобы она хорошо обучалась на ограниченном количестве новых размеченных примеров.
* **dense model** – полносвязная нейронная сеть (будем использовать такие в качестве baseline’а)
* **sparse model** – модель, в которой лишь небольшая часть параметров отличны от нуля. Вообще говоря, именно благодаря таким моделям MoE вообще и существует.
* **Perplexity** - метрика оценки качества моделей, показывающая насколько хорошо модель предсказывает детали тестовой коллекции. Выглядит она так:

$$
  PPL(X) = exp(-\frac{1}{t} \sum_i^t \log p_{\theta}(x_i|x_{0,...,i-1}))
$$

По предыдущей главе мы помним, что существует **Sparsely-gated MoE**, которая умеет для каждой задачи выбирать "исполнителей" и тем самым, за счет общего увеличения количества параметров, она дает существенный прирост в производительности - просто потому что на каждой "подзадаче" она использует лишь малую часть всех параметров. Но вопрос - действительно ли прирост такой большой? 

## Оценка эффективности MoE

Давайте построим структуру экспериментов:

## Модели

Мы будем обучать `decoder-only` трансформеры с таким же количеством параметров и архитектурой, как указано в [статье](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf). Также, для корректного сравнения, мы создадим и обучим MoE аналоги наших моделей. Мы будем использовать в них 512 экспертов, а также на каждой итерации будем использовать "топ-2 эксперта" - то есть отдавать задачу двум экспертам, и после конкатенировать их выходы. Подробнее про архитектуру MoE моделей можно почитать [тут](https://openreview.net/pdf?id=qrwe7XHTmYb).

Ниже вы можете увидеть сводную таблицу с используемыми моделями.

![Models](/Mixture_of_experts_и_применение_к_GPT/3.1.png "Models")

## Данные для предобучения

Тут будут использоваться 6 датасетов, общим объемом _453 gb_, содержащие более _100 миллиардов_ токенов:

* **BookCorpus**: Содержит более 10000 неопубликованных книг (4GB)
* **English Wikipedia**: (12GB)
* **CC-News**: Содержит 63 миллиона вырезок из новостей. Данные принадлежат отрезку "сентябрь 2016 - февраль 2019" (76GB)
* **OpenWebText**: (38GB);
* **CC-Stories**: Cодержит подмножество _СommonCrawl_ данных, которое соответсвует story-like стиль _Winograd schemas_ (31GB);
* **English CC100**: Содержит информацию из _CommonCrawl_ в промежутке "Январь 2018 - Декарь 2018", которая соответствует стилю Википедии (292GB).

## Описание методов

Для начала, мы будем стараться предсказать следующий токен, и оценивать это с помощью _перплексии_. Помимо этого, мы оценим модели как **in-domain**, так и **out-of-domain**:

* **in-domain**: Мы будем использовать данные из верхних датасетов, которые не участвовали в обучении.
* **out-of-domain**: Мы будем использовать данные из _The Pile_

После этого, мы оценим модели на следующих задачах:

* **Zero-shot**: проверим, насколько хорошо модели справляются с задачами, которые раньше вообще не видели
* **Few-shot**: посмотрим, как хорошо они справляются с новыми (незнакомыми) задачами, если им удалось дообучиться лишь на очень ограниченном наборе таких задач
* **Baseline and Benchmarks**: в качестве бейзлайна возьмем числа, которые показала GPT-3 в [более ранней статье](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf), а в качестве набора задач возьмем те, на которых GPT-3 показала положительный прирост в качестве при масштабировании, либо при переходе от _zero-shot_ к _few-shot_.
* **Подходы**:
  * **Priming** - будем подавать модели на вход определенного вида текст (для каждого датасета свой), чтобы она лучше поняла, что мы от нее ждем.
  * **Fine-tuning** - мы дообучим нашу модель на всех данных. Помимо этого, добавим дополнительный линейный слой (особый для каждой задачи) для каждого исполнителя, после чего добавим функцию активации _softmax_.

## Оценка

Наш следующий шаг - провести эксперимент и оценить его результаты.

Ниже можно ознакомиться с результатами, а после них мы подведем итоги:

![Language modeling perplexity](/Mixture_of_experts_и_применение_к_GPT/3.2.png "Language modeling perplexity")

![Estimate of how much more efficient MoEs are relative to dense models in representative datasets](/Mixture_of_experts_и_применение_к_GPT/3.3.png "Estimate of how much more efficient MoEs are relative to dense models in representative datasets")

<img src="/Mixture_of_experts_и_применение_к_GPT/3.4.png" alt="Zero-shot priming accuracy" width="50%"/>

<img src="/Mixture_of_experts_и_применение_к_GPT/3.5.png" alt="Zero-shot priming accuracy averaged across 6 tasks as a function of compute cost" width="50%"/>

![improvements](/Mixture_of_experts_и_применение_к_GPT/3.6.png "improvements")

<img src="/Mixture_of_experts_и_применение_к_GPT/3.7.png" alt="Absolute accuracy improvement going from zero-shot to few-shot" width="50%"/>

# Прикладное применение Mixture of Experts в реальной истории.
Говоря о преимуществах и прочих аспектах, стоит также упомянуть и использование Mixture of Experts в вопросах из реального мира.

В 2022 году в американском штате Мэриленд была проведена Всемирная Конференция по машинному обучению, на которой группа программистов и аналитиков внедрили Mixture of Experts в так называемый GLaM.

## Что такое GLaM?
GLaM (Generalist Language Model) - это обучающая языковая модель, которая имеет способность вырабатывать последовательные тексты на основе заданного контекста. 

Технически GLaM - это продукт OpenAI, созданный на основе GPT (Generative Pre-trained Transformer), и использует большие объемы данных для обучения модели на различных задачах обработки естественного языка. Она может генерировать тексты на основе заданных вводных данных и может быть настроена на разные стили и темы.

## Где применяется GLaM?
GLaM может применяться во многих областях, включая:

1. **Генерация контента**: GLaM может быть использован для автоматического создания статей, новостей, рецензий, контента для блогов и других текстов на основе заданных параметров и тематики.

2. **Чат-боты и виртуальные помощники**: GLaM может быть обучена на исторических беседах или диалогах и использоваться для создания чат-ботов, которые могут эмулировать естественный язык и отвечать на вопросы и комментарии пользователей.

3. **Автоматический перевод**: GLaM может быть использована как базовая модель для систем автоматического перевода и помочь в генерации высококачественных переводов.

4. **Редактирование и корректировка текстов**: GLaM может использоваться для проверки грамматики, стилистики и синтаксических ошибок в текстах, а также предлагать варианты редактирования.

5. **Создание игр**: GLaM может использоваться для создания генеративных текстовых игр, где игроки взаимодействуют с экраном, задавая вопросы или принимая решения, а GLaM генерирует ответы и результаты в соответствии с действиями игрока.

6. **Генерация кода**: GLaM может использоваться для помощи в разработке программного обеспечения, предоставляя синтаксически правильные фрагменты кода в ответ на заданный контекст или запрос.

Однако важно отметить, что использование GLaM может иметь свои ограничения и вызывать опасения о создании неправильной или неэтичной информации. Такие модели должны использоваться с осторожностью и соответствовать установленным правилам и нормам.

## В чём преимущества (показанные в исследованиях 2022 года) GLaM?
Масштабирование языковых моделей с использованием большего количества данных, вычислений и параметров привело к значительному прогрессу в обработке естественного языка.

Таким образом, благодаря масштабированию **GPT-3** смог добиться хороших результатов при выполнении задач контекстного обучения. Однако обучение этих больших плотных моделей требует значительных вычислительных ресурсов.

В проведённых исследованиях 2022 года были предложены и разработаны семейство языковых моделей под названием GLaM, которое использует редко активируемую архитектуру смешанных экспертов для масштабирования емкости модели, а также требует существенно меньших затрат на обучение по сравнению с плотными вариантами. Самый большой GLaM имеет 1,2 триллиона параметров, что примерно в 7 раз больше, чем GPT-3. Он потребляет только 1/3 энергии, используемой для обучения GPT-3, и требует половину вычислительных циклов для вывода, при этом обеспечивая при этом лучшую общую производительность с нулевым, однократным и малократным выполнением 29 задач NLP.

Ниже более детально представлены результаты замеров по применению GPT-3 и GLaM.

<img src="/Mixture_of_experts_и_применение_к_GPT/GPT3:GLAM.png" alt="GPT-3, GLaM" width="50%"/>

Говоря более простыми словами, GLaM превосходит GPT-3 по 21 тесту понимания естественного языка (NLU) и 8 тестам генерации естественного языка (NLG) в среднем, используя при этом около половины FLOP на токен во время вывода и потребляя около трети энергии для обучения.

Дополнительно ниже представлен обзор процентного изменения в эффективности прогнозирования (чем выше, тем лучше) GLaM (64B/64E) по сравнению с GPT-3 (175B) при (а) Zero-Shot, (б) One-Shot и (в) Few-Shot по 7 категориям тестов с 29 общедоступными задачами:

![percent delta](/Mixture_of_experts_и_применение_к_GPT/5.2.png "percent delta")

Каждая полоса на изображениях (a), (b) и (c) представляет одну контрольную категорию. Изображение (d) сравнивает количество FLOP, необходимое для прогнозирования токена и энергопотребления при обучении.

## Как работает GLaM?
Ниже представлена архитектура GLaM:

<img src="/Mixture_of_experts_и_применение_к_GPT/5.3.jpg" alt="GLaM architecture" width="50%"/>

Каждый слой Mixture of Experts (нижний блок) чередуется со слоем Transformer (верхний блок). Для каждого входного токена, например «roses», модуль Gating динамически выбирает двух наиболее релевантных экспертов из 64, которые представлены синей сеткой на слое Mixture of Experts. Средневзвешенное значение результатов этих двух экспертов затем будет передано на верхний уровень преобразователя. Для следующего токена во входной последовательности будут выбраны два разных эксперта.

## Как происходило обучение GLaM?
Для обучения данной модели был создан высококачественный набор данных из 1,6 триллионов токенов, которые представляют широкий спектр вариантов использования естественного языка. Веб-страницы составляют огромное количество данных в данном неразмеченном наборе данных. Однако их качество варьируется от профессионального письма до некачественных комментариев и страниц форума. Также в данных исследованиях был разработан собственный классификатор качества текста для создания высококачественного веб-корпуса из исходного большего необработанного корпуса. Был использован линейный классификатор на основе хеш-функций для скорости вывода. Этот классификатор обучен классифицировать набор тщательно подобранного текста (Википедия, книги и несколько избранных веб-сайтов) и других веб-страниц. Этот классификатор был применён для оценки качества контента веб-страницы. Затем применили этот классификатор, используя распределение Парето, для выборки веб-страниц в соответствии с их оценкой. Это позволяет включать некоторые веб-страницы более низкого качества, чтобы предотвратить систематические ошибки в классификаторе.

Ниже представлен более детально описанный датасет для GLaM:

<img src="/Mixture_of_experts_и_применение_к_GPT/5.4.jpg" alt="GLaM dataset" width="50%"/>


