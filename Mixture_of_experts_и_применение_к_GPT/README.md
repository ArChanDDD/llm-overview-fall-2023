# Mixture of Experts.
Выполнили:
* Максим Романов
* Владислав Понизяйкин
* Глеб Дьяконов
* Александра Павлова
* Георгий Гензе

# Содержание
* пункт 1
* пункт 2
* пункт 3
* пункт 4
* Прикладное применение Mixture of Experts в реальной истории.

(здесь должен быть текст…)

# Прикладное применение Mixture of Experts в реальной истории.
Говоря о преимуществах и прочих аспектах, стоит также упомянуть и использование Mixture of Experts в вопросах из реального мира.

В 2022 году в американском штате Мэриленд была проведена Всемирная Конференция по машинному обучению, на которой группа программистов и аналитиков внедрили Mixture of Experts в так называемый GLaM.

## Что такое GLaM?
GLaM (Generalist Language Model) - это обучающая языковая модель, которая имеет способность вырабатывать последовательные тексты на основе заданного контекста. 

Технически GLaM - это продукт OpenAI, созданный на основе GPT (Generative Pre-trained Transformer), и использует большие объемы данных для обучения модели на различных задачах обработки естественного языка. Она может генерировать тексты на основе заданных вводных данных и может быть настроена на разные стили и темы.

## Где применяется GLaM?
GLaM может применяться во многих областях, включая:

1. **Генерация контента**: GLaM может быть использован для автоматического создания статей, новостей, рецензий, контента для блогов и других текстов на основе заданных параметров и тематики.

2. **Чат-боты и виртуальные помощники**: GLaM может быть обучена на исторических беседах или диалогах и использоваться для создания чат-ботов, которые могут эмулировать естественный язык и отвечать на вопросы и комментарии пользователей.

3. **Автоматический перевод**: GLaM может быть использована как базовая модель для систем автоматического перевода и помочь в генерации высококачественных переводов.

4. **Редактирование и корректировка текстов**: GLaM может использоваться для проверки грамматики, стилистики и синтаксических ошибок в текстах, а также предлагать варианты редактирования.

5. **Создание игр**: GLaM может использоваться для создания генеративных текстовых игр, где игроки взаимодействуют с экраном, задавая вопросы или принимая решения, а GLaM генерирует ответы и результаты в соответствии с действиями игрока.

6. **Генерация кода**: GLaM может использоваться для помощи в разработке программного обеспечения, предоставляя синтаксически правильные фрагменты кода в ответ на заданный контекст или запрос.

Однако важно отметить, что использование GLaM может иметь свои ограничения и вызывать опасения о создании неправильной или неэтичной информации. Такие модели должны использоваться с осторожностью и соответствовать установленным правилам и нормам.

## В чём преимущества (показанные в исследованиях 2022 года) GLaM?
Масштабирование языковых моделей с использованием большего количества данных, вычислений и параметров привело к значительному прогрессу в обработке естественного языка.

Таким образом, благодаря масштабированию **GPT-3** смог добиться хороших результатов при выполнении задач контекстного обучения. Однако обучение этих больших плотных моделей требует значительных вычислительных ресурсов.

В проведённых исследованиях 2022 года были предложены и разработаны семейство языковых моделей под названием GLaM, которое использует редко активируемую архитектуру смешанных экспертов для масштабирования емкости модели, а также требует существенно меньших затрат на обучение по сравнению с плотными вариантами. Самый большой GLaM имеет 1,2 триллиона параметров, что примерно в 7 раз больше, чем GPT-3. Он потребляет только 1/3 энергии, используемой для обучения GPT-3, и требует половину вычислительных циклов для вывода, при этом обеспечивая при этом лучшую общую производительность с нулевым, однократным и малократным выполнением 29 задач NLP.

Ниже более детально представлены результаты замеров по применению GPT-3 и GLaM.

![GPT-3, GLaM](/Mixture_of_experts_и_применение_к_GPT/GPT3:GLAM.png "GPT-3/GLaM")

Говоря более простыми словами, GLaM превосходит GPT-3 по 21 тесту понимания естественного языка (NLU) и 8 тестам генерации естественного языка (NLG) в среднем, используя при этом около половины FLOP на токен во время вывода и потребляя около трети энергии для обучения.

Дополнительно ниже представлен обзор процентного изменения в эффективности прогнозирования (чем выше, тем лучше) GLaM (64B/64E) по сравнению с GPT-3 (175B) при (а) Zero-Shot, (б) One-Shot и (в) Few-Shot по 7 категориям тестов с 29 общедоступными задачами:



Каждая полоса на изображениях (a), (b) и (c) представляет одну контрольную категорию. Изображение (d) сравнивает количество FLOP, необходимое для прогнозирования токена и энергопотребления при обучении.

Как работает GLaM?
Ниже представлена архитектура GLaM:


Каждый слой Mixture of Experts (нижний блок) чередуется со слоем Transformer (верхний блок). Для каждого входного токена, например «roses», модуль Gating динамически выбирает двух наиболее релевантных экспертов из 64, которые представлены синей сеткой на слое Mixture of Experts. Средневзвешенное значение результатов этих двух экспертов затем будет передано на верхний уровень преобразователя. Для следующего токена во входной последовательности будут выбраны два разных эксперта.

Как происходило обучение GLaM?
Для обучения данной модели был создан высококачественный набор данных из 1,6 триллионов токенов, которые представляют широкий спектр вариантов использования естественного языка. Веб-страницы составляют огромное количество данных в данном неразмеченном наборе данных. Однако их качество варьируется от профессионального письма до некачественных комментариев и страниц форума. Также в данных исследованиях был разработан собственный классификатор качества текста для создания высококачественного веб-корпуса из исходного большего необработанного корпуса. Был использован линейный классификатор на основе хеш-функций для скорости вывода. Этот классификатор обучен классифицировать набор тщательно подобранного текста (Википедия, книги и несколько избранных веб-сайтов) и других веб-страниц. Этот классификатор был применён для оценки качества контента веб-страницы. Затем применили этот классификатор, используя распределение Парето, для выборки веб-страниц в соответствии с их оценкой. Это позволяет включать некоторые веб-страницы более низкого качества, чтобы предотвратить систематические ошибки в классификаторе.

Ниже представлен более детально описанный датасет для GLaM:




